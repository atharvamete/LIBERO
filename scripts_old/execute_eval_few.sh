# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep20.pth" \
#     exp_name="eval40_lib10_m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot_ep20" \
#     policy.skill_vae_1.fsq_level=[5,3] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep20.pth" \
#     exp_name="eval40_lib10_m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot_ep20" \
#     policy.skill_vae_1.fsq_level=[8,8] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep20.pth" \
#     exp_name="eval40_lib10_m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot_ep20" \
#     policy.skill_vae_1.fsq_level=[8,6,5] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep20.pth" \
#     exp_name="eval40_lib10_m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot_ep20" \
#     policy.skill_vae_1.fsq_level=[8,8,8] \
#     benchmark_name="LIBERO_10" \



# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep40.pth" \
#     exp_name="eval40_lib10_m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot_ep40" \
#     policy.skill_vae_1.fsq_level=[5,3] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep40.pth" \
#     exp_name="eval40_lib10_m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot_ep40" \
#     policy.skill_vae_1.fsq_level=[8,8] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep40.pth" \
#     exp_name="eval40_lib10_m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot_ep40" \
#     policy.skill_vae_1.fsq_level=[8,6,5] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep40.pth" \
#     exp_name="eval40_lib10_m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot_ep40" \
#     policy.skill_vae_1.fsq_level=[8,8,8] \
#     benchmark_name="LIBERO_10" \



# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep60.pth" \
#     exp_name="eval40_lib10_m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot_ep60" \
#     policy.skill_vae_1.fsq_level=[5,3] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep60.pth" \
#     exp_name="eval40_lib10_m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot_ep60" \
#     policy.skill_vae_1.fsq_level=[8,8] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep60.pth" \
#     exp_name="eval40_lib10_m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot_ep60" \
#     policy.skill_vae_1.fsq_level=[8,6,5] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep60.pth" \
#     exp_name="eval40_lib10_m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot_ep60" \
#     policy.skill_vae_1.fsq_level=[8,8,8] \
#     benchmark_name="LIBERO_10" \



# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep80.pth" \
#     exp_name="eval40_lib10_m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot_ep80" \
#     policy.skill_vae_1.fsq_level=[5,3] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep80.pth" \
#     exp_name="eval40_lib10_m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot_ep80" \
#     policy.skill_vae_1.fsq_level=[8,8] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep80.pth" \
#     exp_name="eval40_lib10_m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot_ep80" \
#     policy.skill_vae_1.fsq_level=[8,6,5] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep80.pth" \
#     exp_name="eval40_lib10_m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot_ep80" \
#     policy.skill_vae_1.fsq_level=[8,8,8] \
#     benchmark_name="LIBERO_10" \


# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep100.pth" \
#     exp_name="eval40_lib10_m4no_32_f16_k3s4_tt_n6d384_off0_ep20_5shot_ep100" \
#     policy.skill_vae_1.fsq_level=[5,3] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep100.pth" \
#     exp_name="eval40_lib10_m4no_32_f64_k3s4_tt_n6d384_off0_ep20_5shot_ep100" \
#     policy.skill_vae_1.fsq_level=[8,8] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep100.pth" \
#     exp_name="eval40_lib10_m4no_32_f256_k3s4_tt_n6d384_off0_ep20_5shot_ep100" \
#     policy.skill_vae_1.fsq_level=[8,6,5] \
#     benchmark_name="LIBERO_10" \

# sbatch slurm/eval.sbatch python libero/lifelong/skill_policy_eval.py \
#     pretrain_model_path="/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_finetune_clip/LIBERO_10/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot/run_001/multitask_model_ep100.pth" \
#     exp_name="eval40_lib10_m4no_32_f512_k3s4_tt_n6d384_off0_ep20_5shot_ep100" \
#     policy.skill_vae_1.fsq_level=[8,8,8] \
#     benchmark_name="LIBERO_10" \