# @package _global_

defaults:
  - data: default
  - policy: vq_bet
  - train: default
  - eval: default
  - lifelong: multitask
  - test: null
  - _self_

# change defaults
data:
  seq_len: 14
  obs_seq_len: 10
eval:
  eval: true
  n_eval: 20
  num_procs: 6
  batch_size: 128
  num_workers: 6
policy:
  skill_block_size: 5
  mpc_horizon: 5

pretrain_vqvae_path: null
exp_name: "eval40_lib10_vqbet_a5_l1h128_c16_n2_n6d120" # n6d384: GPT model num_heads=num_layers=6, d_model=384
seed: 10000
use_wandb: false
wandb_project: "diff-skill-libero"
folder: null # use default path
bddl_folder: null # use default path
init_states_folder: null # use default path
load_previous_model: false
tune_decoder: false
device: "cuda"
task_embedding_format: "clip"
task_embedding_one_hot_offset: 1
pretrain: false
pretrain_model_path: "/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_clip/LIBERO_10/Multitask/VQBet_Model/ResnetEncoder/vqbet_a5_l1h128_c16_n2_n6d120/run_001/multitask_model_ep160.pth"
benchmark_name: "LIBERO_10"
