# @package _global_

defaults:
  - data: default
  - policy: diffusion_policy
  - train: default
  - eval: default
  - lifelong: multitask
  - test: null
  - _self_

# change defaults
data:
  seq_len: 32
  obs_seq_len: 1
  few_shot_demos: [1, 5, 10, 20, 45] # demo numbers for few-shot learning, use null for using all demos
eval:
  eval: false
  eval_every: 10
train:
  batch_size: 256
  num_workers: 6
  n_epochs: 200
policy:
  skill_block_size: 32

exp_name: "m4_64_f4_k3s4_tt_n6d384_5shot_decobs" # n6d384: GPT model num_heads=num_layers=6, d_model=384, 5-shot, decoder not frozen
seed: 10000
use_wandb: false
wandb_project: "diff-skill-libero"
folder: null # use default path
bddl_folder: null # use default path
init_states_folder: null # use default path
load_previous_model: false
pretrain_skillVAE_path: null
device: "cuda"
task_embedding_format: "clip"
task_embedding_one_hot_offset: 1
pretrain: false
pretrain_model_path: "/satassdscratch/amete7/LIBERO/experiments_clip/LIBERO_90/Multitask/SkillGPT_Model/ResnetEncoder/m4_64_f4_k3s4_tt_n6d384/run_001/multitask_model_ep100.pth"
benchmark_name: "LIBERO_10"
