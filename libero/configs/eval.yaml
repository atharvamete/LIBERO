# @package _global_

defaults:
  - data: default
  - policy: skill_GPT
  - train: default
  - eval: default
  - lifelong: multitask
  - test: null
  # - override policy/image_encoder: dino_encoder.yaml
  # - override policy/data_augmentation@policy.color_aug: identity_aug.yaml
  # - override policy/data_augmentation@policy.color_aug: identity_aug.yaml
  - _self_

# change defaults
data:
  seq_len: 32
  obs_seq_len: 1
eval:
  eval: true
  n_eval: 50
  num_procs: 5
  batch_size: 128
  num_workers: 4

tune_decoder: false
save_video: false
render_h: 128
render_w: 128

pretrain_skillVAE_path: null
pretrain_model_path: "/storage/home/hcoda1/0/amete7/p-agarg35-0/diff-skill/LIBERO/experiments_clip/LIBERO_90/Multitask/SkillGPT_Model/ResnetEncoder/m4no_32_f4_k3s4_tt_n6d384_off0/run_001/multitask_model_ep100.pth"
exp_name: "m4no_videos"
seed: 1
use_wandb: false
wandb_project: "diff-skill-libero"
folder: null # use default path
bddl_folder: null # use default path
init_states_folder: null # use default path
load_previous_model: false
device: "cuda"
task_embedding_format: "clip"
task_embedding_one_hot_offset: 1
pretrain: false
benchmark_name: "LIBERO_10"
